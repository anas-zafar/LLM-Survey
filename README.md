# Large language models: A comprehensive survey of its applications, challenges, limitations, and future prospects (Updated 2025)

The Large Language Models Survey repository is a comprehensive compendium dedicated to the exploration and understanding of Large Language Models (LLMs). It houses an assortment of resources including research papers, blog posts, tutorials, code examples, and more to provide an in-depth look at the progression, methodologies, and applications of LLMs. This repo is an invaluable resource for AI researchers, data scientists, or enthusiasts interested in the advancements and inner workings of LLMs. We encourage contributions from the wider community to promote collaborative learning and continue pushing the boundaries of LLM research.

## Timeline of LLMs
![evolutionv1 1](https://github.com/anas-zafar/LLM-Survey/blob/main/Images/evolutionv1.1.PNG)

## List of LLMs (Updated July 2025)

| Language Model | Release Date | Checkpoints | Paper/Blog | Params (B) | Context Length | Licence | Try it |
|---|---|---|---|---|---|---|---|
| **2025 Latest Models** |
| Grok 3 | 2025/02 | [Grok 3](https://x.ai/) | [Grok 3 Beta — The Age of Reasoning Agents](https://x.ai/news/grok-3) | 314 active (1M+ total) | 1M tokens | Proprietary | [xAI Platform](https://grok.com) |
| Grok 3 Mini | 2025/02 | [Grok 3 Mini](https://x.ai/) | [Grok 3 Beta](https://x.ai/news/grok-3) | Smaller variant | 1M tokens | Proprietary | [xAI Platform](https://grok.com) |
| Llama 4 Scout | 2025/04 | [Llama 4 Scout](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E) | [The Llama 4 herd: The beginning of a new era](https://ai.meta.com/blog/llama-4-multimodal-intelligence/) | 17B active (109B total) | 10M tokens | [Llama 4 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE) | [HuggingFace](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E) |
| Llama 4 Maverick | 2025/04 | [Llama 4 Maverick](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E) | [The Llama 4 herd](https://ai.meta.com/blog/llama-4-multimodal-intelligence/) | 17B active (400B total) | 1M tokens | [Llama 4 Community License](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE) | [HuggingFace](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E) |
| Llama 4 Behemoth | 2025/04 (Training) | [In Training](https://ai.meta.com/blog/llama-4-multimodal-intelligence/) | [The Llama 4 herd](https://ai.meta.com/blog/llama-4-multimodal-intelligence/) | 288B active (~2T total) | TBD | TBD | TBD |
| Qwen 3 | 2025/04 | [Qwen 3 Family](https://huggingface.co/collections/Qwen/qwen3-6633ebacb5c0a53ce76e1089) | [Alibaba unveils Qwen3](https://techcrunch.com/2025/04/28/alibaba-unveils-qwen-3-a-family-of-hybrid-ai-reasoning-models/) | 0.6B - 235B | 32K - 131K tokens | Apache 2.0 | [Qwen Chat](https://chat.qwen.ai/) |
| Qwen 3-235B-A22B | 2025/04 | [Qwen 3-235B](https://huggingface.co/Qwen/Qwen3-235B-A22B) | [Qwen 3 Blog](https://simonwillison.net/2025/Apr/29/qwen-3/) | 22B active (235B total) | 131K tokens | Apache 2.0 | [Qwen Chat](https://chat.qwen.ai/) |
| DeepSeek-R1 | 2025/01 | [DeepSeek-R1](https://huggingface.co/deepseek-ai/DeepSeek-R1) | [DeepSeek-R1: Incentivizing Reasoning Capability](https://arxiv.org/abs/2501.12948) | 37B active (671B total) | 128K tokens | MIT | [DeepSeek Platform](https://chat.deepseek.com/) |
| DeepSeek-R1-Zero | 2025/01 | [DeepSeek-R1-Zero](https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero) | [DeepSeek-R1 Paper](https://arxiv.org/abs/2501.12948) | 37B active (671B total) | 128K tokens | MIT | [DeepSeek Platform](https://chat.deepseek.com/) |
| DeepSeek-R1-0528 | 2025/05 | [DeepSeek-R1-0528](https://huggingface.co/deepseek-ai/DeepSeek-R1-0528) | [DeepSeek Updates](https://www.bentoml.com/blog/the-complete-guide-to-deepseek-models-from-v3-to-r1-and-beyond) | 37B active (671B total) | 128K tokens | MIT | [DeepSeek Platform](https://chat.deepseek.com/) |
| o3 | 2025/04 | [o3](https://platform.openai.com/docs/models/o3) | [Introducing OpenAI o3 and o4-mini](https://openai.com/index/introducing-o3-and-o4-mini/) | Undisclosed | 200K tokens | Proprietary | [ChatGPT](https://chat.openai.com/) |
| o3-mini | 2025/01 | [o3-mini](https://platform.openai.com/docs/models/o3-mini) | [Launching o3-mini in the API](https://community.openai.com/t/launching-o3-mini-in-the-api/1109387) | Undisclosed | 200K tokens | Proprietary | [ChatGPT](https://chat.openai.com/) |
| o4-mini | 2025/04 | [o4-mini](https://platform.openai.com/docs/models/o4-mini) | [Introducing o3 and o4-mini](https://openai.com/index/introducing-o3-and-o4-mini/) | Undisclosed | 200K tokens | Proprietary | [ChatGPT](https://chat.openai.com/) |
| Claude Sonnet 4 | 2025/05 | [Claude Sonnet 4](https://docs.anthropic.com/en/docs/about-claude/models/overview) | [Introducing Claude 4](https://www.anthropic.com/news/claude-4) | Undisclosed | 200K tokens | Proprietary | [Claude.ai](https://claude.ai/) |
| Claude Opus 4 | 2025/05 | [Claude Opus 4](https://docs.anthropic.com/en/docs/about-claude/models/overview) | [Introducing Claude 4](https://www.anthropic.com/news/claude-4) | Undisclosed | 200K tokens | Proprietary | [Claude.ai](https://claude.ai/) |
| Gemini 2.5 Pro | 2025/03 | [Gemini 2.5 Pro](https://ai.google.dev/gemini-api/docs/models) | [Gemini 2.5: Our newest Gemini model with thinking](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/) | Undisclosed | 1M tokens | Proprietary | [Gemini](https://gemini.google.com/) |
| Gemini 2.5 Flash | 2025/03 | [Gemini 2.5 Flash](https://ai.google.dev/gemini-api/docs/models) | [Gemini 2.5 Updates](https://developers.googleblog.com/en/gemini-2-5-thinking-model-updates/) | Undisclosed | 1M tokens | Proprietary | [Gemini](https://gemini.google.com/) |
| Gemini 2.5 Flash-Lite | 2025/06 | [Gemini 2.5 Flash-Lite](https://ai.google.dev/gemini-api/docs/models) | [Gemini 2.5: Updates to thinking models](https://developers.googleblog.com/en/gemini-2-5-thinking-model-updates/) | Undisclosed | 1M tokens | Proprietary | [Gemini](https://gemini.google.com/) |
| **Previous Generation Models** |
| T5           | 2019/10 |[T5 & Flan-T5](https://github.com/google-research/t5x/blob/main/docs/models.md#flan-t5-checkpoints), [Flan-T5-xxl (HF)](https://huggingface.co/google/flan-t5-xxl)      | [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://github.com/google-research/text-to-text-transfer-transformer#released-model-checkpoints) | 0.06 - 11       | [512](https://discuss.huggingface.co/t/does-t5-truncate-input-longer-than-512-internally/3602) | Apache 2.0         | [T5-Large](https://github.com/slai-labs/get-beam/tree/main/examples/t5)                                               |
| UL2          | 2022/10 | [UL2 & Flan-UL2](https://github.com/google-research/google-research/tree/master/ul2#checkpoints), [Flan-UL2 (HF)](https://huggingface.co/google/flan-ul2)          | [UL2 20B: An Open Source Unified Language Learner](https://ai.googleblog.com/2022/10/ul2-20b-open-source-unified-language.html)                                                       | 20             | [512, 2048](https://huggingface.co/google/flan-ul2#tldr) | Apache 2.0         |                                                                                                                       |
| Cohere  | 2022/06 | [Checkpoint](https://cohere.ai/) | [Code](https://cohere.ai/)     | 54      | [4096](https://cohere.ai/)  | [Model](https://cohere.ai/)   | [Website](https://cohere.ai/) | 
| Cerebras-GPT | 2023/03 | [Cerebras-GPT](https://huggingface.co/cerebras)                                           | [Cerebras-GPT: A Family of Open, Compute-efficient, Large Language Models](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/) ([Paper](https://arxiv.org/abs/2304.03208)) | 0.111 - 13      | [2048](https://huggingface.co/cerebras/Cerebras-GPT-13B#model-details) | Apache 2.0         | [Cerebras-GPT-1.3B](https://github.com/slai-labs/get-beam/tree/main/examples/cerebras-gpt)                            |
| Open Assistant (Pythia family) | 2023/03 | [OA-Pythia-12B-SFT-8](https://huggingface.co/OpenAssistant/pythia-12b-sft-v8-7k-steps), [OA-Pythia-12B-SFT-4](https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5), [OA-Pythia-12B-SFT-1](https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b) | [Democratizing Large Language Model Alignment](https://arxiv.org/abs/2304.07327) | 12     | [2048](https://huggingface.co/OpenAssistant/pythia-12b-sft-v8-7k-steps/blob/main/config.json)  | Apache 2.0                | [Pythia-2.8B](https://github.com/slai-labs/get-beam/tree/main/examples/pythia)                                        |
| Pythia       | 2023/04 | [pythia 70M - 12B](https://github.com/EleutherAI/pythia)                                   | [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/abs/2304.01373)                                                                    | 0.07 - 12       | [2048](https://arxiv.org/pdf/2304.01373.pdf) | Apache 2.0         |                                                                                                                       |
| Dolly        | 2023/04 | [dolly-v2-12b](https://huggingface.co/databricks/dolly-v2-12b)                            | [Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)             | 3, 7, 12     | [2048](https://github.com/databrickslabs/dolly#dolly) | MIT                |                                                                                                                       |
| DLite | 2023/05 | [dlite-v2-1_5b](https://huggingface.co/aisquared/dlite-v2-1_5b) | [Announcing DLite V2: Lightweight, Open LLMs That Can Run Anywhere](https://medium.com/ai-squared/announcing-dlite-v2-lightweight-open-llms-that-can-run-anywhere-a852e5978c6e) | 0.124 - 1.5 | [1024](https://huggingface.co/aisquared/dlite-v2-1_5b/blob/main/config.json) | Apache 2.0         | [DLite-v2-1.5B](https://github.com/slai-labs/get-beam/tree/main/examples/dlite-v2)                                    |
| RWKV         | 2021/08| [RWKV, ChatRWKV](https://github.com/BlinkDL/RWKV-LM#rwkv-parallelizable-rnn-with-transformer-level-llm-performance-pronounced-as-rwakuv-from-4-major-params-r-w-k-v) | [The RWKV Language Model (and my LM tricks)](https://github.com/BlinkDL/RWKV-LM)                                           | 0.1 - 14      | [infinity (RNN)](https://github.com/BlinkDL/RWKV-LM#rwkv-parallelizable-rnn-with-transformer-level-llm-performance-pronounced-as-rwakuv-from-4-major-params-r-w-k-v) | Apache 2.0         |                                                                                                                       |
| GPT-J-6B | 2023/06 | [GPT-J-6B](https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b), [GPT4All-J](https://github.com/nomic-ai/gpt4all#raw-model) | [GPT-J-6B: 6B JAX-Based Transformer](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/) | 6 | [2048](https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b) | Apache 2.0 |                                                                                                                       |
| GPT-NeoX-20B | 2022/04 | [GPT-NEOX-20B](https://huggingface.co/EleutherAI/gpt-neox-20b) | [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745) | 20 | [2048](https://huggingface.co/EleutherAI/gpt-neox-20b) | Apache 2.0 |                                                                                                                       |
| Bloom | 2022/11 | [Bloom](https://huggingface.co/bigscience/bloom) | [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100) | 176 | [2048](https://huggingface.co/bigscience/bloom) |  [OpenRAIL-M v1](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement) |                                                                                                                       |
| StableLM-Alpha | 2023/04 | [StableLM-Alpha](https://github.com/Stability-AI/StableLM#stablelm-alpha) | [Stability AI Launches the First of its StableLM Suite of Language Models](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models) | 3 - 65 | [4096](https://github.com/Stability-AI/StableLM#stablelm-alpha) | CC BY-SA-4.0 |                                                                                                                       |
| FastChat-T5 | 2023/04 | [fastchat-t5-3b-v1.0](https://huggingface.co/lmsys/fastchat-t5-3b-v1.0) | [We are excited to release FastChat-T5: our compact and commercial-friendly chatbot!](https://twitter.com/lmsysorg/status/1652037026705985537?s=20) | 3 | [512](https://huggingface.co/lmsys/fastchat-t5-3b-v1.0/blob/main/config.json) | Apache 2.0 |                                                                                                                       |
| h2oGPT | 2023/05 | [h2oGPT](https://github.com/h2oai/h2ogpt) | [Building the World's Best Open-Source Large Language Model: H2O.ai's Journey](https://h2o.ai/blog/building-the-worlds-best-open-source-large-language-model-h2o-ais-journey/) | 12 - 20 | [256 - 2048](https://huggingface.co/h2oai) | Apache 2.0 |                                                                                                                       |
| MPT-7B | 2023/05 | [MPT-7B](https://huggingface.co/mosaicml/mpt-7b), [MPT-7B-Instruct](https://huggingface.co/mosaicml/mpt-7b-instruct) | [Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs](https://www.mosaicml.com/blog/mpt-7b) | 7 | [84k (ALiBi)](https://huggingface.co/mosaicml/mpt-7b#how-is-this-model-different) | Apache 2.0, CC BY-SA-3.0 |                                          
| PanGU-Σ | 2023/3 | [PanGU](https://arxiv.org/abs/2303.10845) | [Model](https://arxiv.org/abs/2303.10845) | 1085 | [-](https://arxiv.org/abs/2303.10845) | Model | [Page](https://arxiv.org/abs/2303.10845) |                                                                             |
| RedPajama-INCITE | 2023/05 | [RedPajama-INCITE](https://huggingface.co/togethercomputer) | [Releasing 3B and 7B RedPajama-INCITE family of models including base, instruction-tuned & chat models](https://www.together.xyz/blog/redpajama-models-v1) | 3 - 7 | [2048](https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-7B-v0.1/blob/157bf3174feebb67f37e131ea68f84dee007c687/config.json#L13) | Apache 2.0 | [RedPajama-INCITE-Instruct-3B-v1](https://github.com/slai-labs/get-beam/tree/main/examples/redpajama-incite-instruct) |
| OpenLLaMA | 2023/05 | [open_llama_3b](https://huggingface.co/openlm-research/open_llama_3b), [open_llama_7b](https://huggingface.co/openlm-research/open_llama_7b), [open_llama_13b](https://huggingface.co/openlm-research/open_llama_13b) | [OpenLLaMA: An Open Reproduction of LLaMA](https://github.com/openlm-research/open_llama) | 3, 7 | [2048](https://huggingface.co/h2oai) | Apache 2.0 | [OpenLLaMA-7B-Preview_200bt](https://github.com/slai-labs/get-beam/tree/main/examples/openllama)                      |
| Falcon | 2023/05 | [Falcon-180B](https://huggingface.co/tiiuae/falcon-180B), [Falcon-40B](https://huggingface.co/tiiuae/falcon-40b), [Falcon-7B](https://huggingface.co/tiiuae/falcon-7b) | [The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only](https://arxiv.org/abs/2306.01116) | 180, 40, 7 | [2048](https://huggingface.co/tiiuae/falcon-7b/blob/main/config.json) | Apache 2.0 | 
| MPT-30B | 2023/06 | [MPT-30B](https://huggingface.co/mosaicml/mpt-30b), [MPT-30B-instruct](https://huggingface.co/mosaicml/mpt-30b-instruct) | [MPT-30B: Raising the bar for open-source foundation models](https://www.mosaicml.com/blog/mpt-30b) | 30 | [8192](https://huggingface.co/mosaicml/mpt-30b/blob/main/config.json) | Apache 2.0, CC BY-SA-3.0 | [MPT 30B inference code using CPU](https://github.com/abacaj/mpt-30B-inference) |
| LLaMA 2  | 2023/06 | [LLaMA 2 Weights](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://scontent-ham3-1.xx.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=qhK-ahCbkBMAX94XV2X&_nc_ht=scontent-ham3-1.xx&oh=00_AfDB7dN8momft9nkv8X0gqrZdEnKltVjPOxhKBm0XLRinA&oe=64BE66FF)      | 7 - 70       | [4096](https://scontent-ham3-1.xx.fbcdn.net/v/t39.2365-6/10000000_662098952474184_2584067087619170692_n.pdf?_nc_cat=105&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=qhK-ahCbkBMAX94XV2X&_nc_ht=scontent-ham3-1.xx&oh=00_AfDB7dN8momft9nkv8X0gqrZdEnKltVjPOxhKBm0XLRinA&oe=64BE66FF)  | [Custom](https://github.com/facebookresearch/llama/blob/main/LICENSE) Free if you have under 700M users and you cannot use LLaMA outputs to train other LLMs besides LLaMA and its derivatives   | [HuggingChat](https://huggingface.co/blog/llama2#demo) |  
| LLaMA 3 | 2024/04 | [LLaMA 3 Weights](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) | [Introducing Meta Llama 3](https://ai.meta.com/blog/meta-llama-3/) | 8 - 405 | [8K-128K](https://ai.meta.com/blog/meta-llama-3/) | [Custom](https://github.com/facebookresearch/llama/blob/main/LICENSE) | [HuggingChat](https://huggingface.co/chat/) |
| LLaMA 3.3 | 2024/12 | [LLaMA 3.3 70B](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [Llama 3.3 70B](https://ai.meta.com/blog/llama-3-3-70b/) | 70 | [128K](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) | [Custom](https://github.com/facebookresearch/llama/blob/main/LICENSE) | [HuggingChat](https://huggingface.co/chat/) |
| OpenLM  | 2023/09 | [OpenLM 1B](https://huggingface.co/mlfoundations/open_lm_1B), [OpenLM 7B](https://huggingface.co/mlfoundations/open_lm_7B_1.25T) | [Open LM:  a minimal but performative language modeling (LM) repository](https://github.com/mlfoundations/open_lm#pretrained-models)      | 1, 7       | [2048](https://github.com/mlfoundations/open_lm/blob/main/open_lm/model_configs/open_lm_7b.json)  | MIT   |      |  
| Mistral 7B | 2023/09 | [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1), [Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1) | [Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/) | 7 | [4096-16K with Sliding Windows](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/blob/main/config.json)| Apache 2.0 | [Mistral Transformer](https://github.com/mistralai/mistral-src)
| Mistral Large 2 | 2024/07 | [Mistral Large 2](https://huggingface.co/mistralai/Mistral-Large-Instruct-2407) | [Mistral Large 2](https://mistral.ai/news/mistral-large-2407/) | 123 | [128K](https://huggingface.co/mistralai/Mistral-Large-Instruct-2407) | [Mistral Research License](https://github.com/mistralai/mistral-common/blob/main/LICENSE) | [Mistral Platform](https://chat.mistral.ai/)
| Mistral Medium | 2025/05 | [Mistral Medium](https://mistral.ai/) | [Mistral AI](https://mistral.ai/) | Undisclosed | 128K | Proprietary | [Mistral Platform](https://chat.mistral.ai/)
| OpenHermes | 2023/09 | [OpenHermes-7B](https://huggingface.co/teknium/OpenHermes-7B), [OpenHermes-13B](https://huggingface.co/teknium/OpenHermes-13B) | [Nous Research](https://nousresearch.com/) | 7, 13 | [4096](https://huggingface.co/teknium/OpenHermes-13B/blob/main/config.json)| MIT | [OpenHermes-V2 Finetuned on Mistral 7B](https://huggingface.co/spaces/artificialguybr/OPENHERMES-2)
| SOLAR | 2023/12 | [Solar-10.7B](https://huggingface.co/upstage/SOLAR-10.7B-v1.0) | [Upstage](https://arxiv.org/abs/2312.15166) | 10.7 | [4096](https://huggingface.co/upstage/SOLAR-10.7B-v1.0/blob/main/config.json)| apache-2.0 | |
| phi-2 | 2023/12 | [phi-2 2.7B](https://huggingface.co/microsoft/phi-2) | [Microsoft](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) | 2.7 | [2048](https://huggingface.co/microsoft/phi-2/blob/main/config.json)| MIT | |
| Phi-3 | 2024/04 | [Phi-3 Family](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3) | [Microsoft Phi-3](https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/) | 3.8 - 14 | [4K-128K](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct) | MIT | [Azure AI Studio](https://azure.microsoft.com/en-us/products/ai-studio/)
| DeepSeek-V3 | 2024/12 | [DeepSeek-V3](https://huggingface.co/deepseek-ai/DeepSeek-V3) | [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437) | 37B active (671B total) | 128K tokens | MIT | [DeepSeek Platform](https://chat.deepseek.com/)
| DeepSeek-V3-0324 | 2025/03 | [DeepSeek-V3-0324](https://huggingface.co/deepseek-ai/DeepSeek-V3-0324) | [DeepSeek Updates](https://www.bentoml.com/blog/the-complete-guide-to-deepseek-models-from-v3-to-r1-and-beyond) | 37B active (671B total) | 128K tokens | MIT | [DeepSeek Platform](https://chat.deepseek.com/)
| Qwen 2.5 | 2024/09 | [Qwen 2.5 Family](https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e) | [Qwen2.5: A Party of Foundation Models](https://qwenlm.github.io/blog/qwen2.5/) | 0.5B - 72B | [32K-128K](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct) | Apache 2.0 | [Qwen Chat](https://chat.qwen.ai/)
| Qwen 2.5-Max | 2025/01 | [Qwen 2.5-Max](https://huggingface.co/Qwen/Qwen2.5-Max) | [Alibaba releases AI model it says surpasses DeepSeek](https://www.reuters.com/technology/artificial-intelligence/alibaba-releases-ai-model-it-claims-surpasses-deepseek-v3-2025-01-29/) | Undisclosed | 128K | Proprietary | [Qwen Chat](https://chat.qwen.ai/)
| QwQ-32B | 2024/11 | [QwQ-32B-Preview](https://huggingface.co/Qwen/QwQ-32B-Preview) | [QwQ-32B Technical Report](https://arxiv.org/abs/2411.20213) | 32 | [32K](https://huggingface.co/Qwen/QwQ-32B-Preview) | Apache 2.0 | [Qwen Chat](https://chat.qwen.ai/)
| **Coding-Specialized Models** |
| SantaCoder | 2023/01 | [santacoder](https://huggingface.co/bigcode/santacoder) |[SantaCoder: don't reach for the stars!](https://arxiv.org/abs/2301.03988) | 1.1 | [2048](https://huggingface.co/bigcode/santacoder/blob/main/README.md#model-summary)                | [OpenRAIL-M v1](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement) | [SantaCoder](https://github.com/slai-labs/get-beam/tree/main/examples/santacoder)         |
| StarCoder | 2023/05 | [starcoder](https://huggingface.co/bigcode/starcoder) | [StarCoder: A State-of-the-Art LLM for Code](https://huggingface.co/blog/starcoder), [StarCoder: May the source be with you!](https://drive.google.com/file/d/1cN-b9GnWtHzQRoE7M7gAEyivY0kl4BYs/view) | 1.1-15 | [8192](https://huggingface.co/bigcode/starcoder#model-summary)                         | [OpenRAIL-M v1](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement) |                                                                                           |
| StarChat Alpha | 2023/05 | [starchat-alpha](https://huggingface.co/HuggingFaceH4/starchat-alpha) | [Creating a Coding Assistant with StarCoder](https://huggingface.co/blog/starchat-alpha) | 16 | [8192](https://huggingface.co/bigcode/starcoder#model-summary)  | [OpenRAIL-M v1](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement) |                                                                                           |
| Replit Code | 2023/05 | [replit-code-v1-3b](https://huggingface.co/replit/replit-code-v1-3b) | [Training a SOTA Code LLM in 1 week and Quantifying the Vibes — with Reza Shabani of Replit](https://www.latent.space/p/reza-shabani#details) | 2.7 | [infinity? (ALiBi)](https://huggingface.co/replit/replit-code-v1-3b#model-description) | CC BY-SA-4.0 | [Replit-Code-v1-3B](https://github.com/slai-labs/get-beam/tree/main/examples/replit-code) |
| CodeGen2 | 2023/04 | [codegen2 1B-16B](https://github.com/salesforce/CodeGen2) | [CodeGen2: Lessons for Training LLMs on Programming and Natural Languages](https://arxiv.org/abs/2305.02309) | 1 - 16 | [2048](https://arxiv.org/abs/2305.02309) | [Apache 2.0](https://github.com/salesforce/CodeGen2/blob/main/LICENSE)|                                                                                           |
| CodeT5+ | 2023/05 | [CodeT5+](https://github.com/salesforce/CodeT5/tree/main/CodeT5+)     | [CodeT5+: Open Code Large Language Models for Code Understanding and Generation](https://arxiv.org/abs/2305.07922) | 0.22 - 16 | [512](https://arxiv.org/abs/2305.07922)                                                                                | [BSD-3-Clause](https://github.com/salesforce/CodeT5/blob/main/LICENSE.txt)                                                           | [Codet5+-6B](https://github.com/slai-labs/get-beam/tree/main/examples/codeT5%2B)          |
| XGen-7B | 2023/06 | [XGen-7B-8K-Base](https://huggingface.co/Salesforce/xgen-7b-8k-base) | [Long Sequence Modeling with XGen: A 7B LLM Trained on 8K Input Sequence Length](https://blog.salesforceairesearch.com/xgen/) | 7 | [8192](https://huggingface.co/Salesforce/xgen-7b-8k-base/blob/main/config.json) | [Apache 2.0](https://github.com/salesforce/xgen/blob/main/LICENSE) | 
| CodeGen2.5 | 2023/07 | [CodeGen2.5-7B-multi](https://huggingface.co/Salesforce/codegen25-7b-multi) | [CodeGen2.5: Small, but mighty](https://blog.salesforceairesearch.com/codegen25/) | 7 | [2048](https://huggingface.co/Salesforce/codegen25-7b-multi/blob/main/config.json) | [Apache 2.0](https://huggingface.co/Salesforce/codegen25-7b-multi/blob/main/README.md) | 
| DeciCoder-1B | 2023/08 | [DeciCoder-1B](https://huggingface.co/Deci/DeciCoder-1b#how-to-use) | [Introducing DeciCoder: The New Gold Standard in Efficient and Accurate Code Generation](https://deci.ai/blog/decicoder-efficient-and-accurate-code-generation-llm/) | 1.1 | [2048](https://huggingface.co/Deci/DeciCoder-1b#model-architecture) | Apache 2.0 |  [DeciCoder Demo](https://huggingface.co/spaces/Deci/DeciCoder-Demo)|
| Code Llama  | 2023/08 | [Inference Code for CodeLlama models](https://github.com/facebookresearch/codellama) | [Code Llama: Open Foundation Models for Code](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/)     | 7 - 34       | [4096](https://scontent-zrh1-1.xx.fbcdn.net/v/t39.2365-6/369856151_1754812304950972_1159666448927483931_n.pdf?_nc_cat=107&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=wURKmnWKaloAX-ib5XW&_nc_ht=scontent-zrh1-1.xx&oh=00_AfAN1GB2K_XwIz54PqXTr-dhilI3CfCwdQoaLMyaYEEECg&oe=64F0A68F)  | [Model](https://github.com/facebookresearch/llama/blob/main/LICENSE)   | [HuggingChat](https://huggingface.co/blog/codellama) | 
| **Legacy Models** |
| Sparrow  | 2022/09 | [Inference Code](http://arxiv.org/abs/2209.14375v1) | [Code](http://arxiv.org/abs/2209.14375v1)     | 70       | [4096](http://arxiv.org/abs/2209.14375v1)  | [Model](http://arxiv.org/abs/2209.14375v1)   | [Webpage](http://arxiv.org/abs/2209.14375v1) | 
| Koala  | 2023/04 | [Inference Code](https://bair.berkeley.edu/blog/2023/04/03/koala/) | [Code](https://bair.berkeley.edu/blog/2023/04/03/koala/)     | 13       | [4096](https://bair.berkeley.edu/blog/2023/04/03/koala/)  | [Model](https://bair.berkeley.edu/blog/2023/04/03/koala/)   | [Webpage](https://bair.berkeley.edu/blog/2023/04/03/koala/) |
| GPT-4 | 2023/03 | [API Access](https://platform.openai.com/docs/models/gpt-4) | [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774) | Undisclosed | [8K-128K](https://platform.openai.com/docs/models/gpt-4) | Proprietary | [ChatGPT](https://chat.openai.com/) |
| GPT-4.5 | 2024/06 | [API Access](https://platform.openai.com/docs/models/gpt-4) | [GPT-4.5 Updates](https://openai.com/index/gpt-4-5-updates/) | Undisclosed | [128K](https://platform.openai.com/docs/models/gpt-4) | Proprietary | [ChatGPT](https://chat.openai.com/) |
| GPT-4o | 2024/05 | [API Access](https://platform.openai.com/docs/models/gpt-4o) | [Hello GPT-4o](https://openai.com/index/hello-gpt-4o/) | Undisclosed | [128K](https://platform.openai.com/docs/models/gpt-4o) | Proprietary | [ChatGPT](https://chat.openai.com/) |
| o1 | 2024/09 | [o1](https://platform.openai.com/docs/models/o1) | [Learning to Reason with LLMs](https://openai.com/index/learning-to-reason-with-llms/) | Undisclosed | [200K](https://platform.openai.com/docs/models/o1) | Proprietary | [ChatGPT](https://chat.openai.com/) |
| o1-mini | 2024/09 | [o1-mini](https://platform.openai.com/docs/models/o1) | [Learning to Reason with LLMs](https://openai.com/index/learning-to-reason-with-llms/) | Undisclosed | [128K](https://platform.openai.com/docs/models/o1) | Proprietary | [ChatGPT](https://chat.openai.com/) |
| Claude 3 Haiku | 2024/03 | [Claude 3 Family](https://docs.anthropic.com/en/docs/about-claude/models) | [Introducing the next generation of Claude](https://www.anthropic.com/news/claude-3-family) | Undisclosed | [200K](https://docs.anthropic.com/en/docs/about-claude/models) | Proprietary | [Claude.ai](https://claude.ai/) |
| Claude 3 Sonnet | 2024/03 | [Claude 3 Family](https://docs.anthropic.com/en/docs/about-claude/models) | [Introducing the next generation of Claude](https://www.anthropic.com/news/claude-3-family) | Undisclosed | [200K](https://docs.anthropic.com/en/docs/about-claude/models) | Proprietary | [Claude.ai](https://claude.ai/) |
| Claude 3 Opus | 2024/03 | [Claude 3 Family](https://docs.anthropic.com/en/docs/about-claude/models) | [Introducing the next generation of Claude](https://www.anthropic.com/news/claude-3-family) | Undisclosed | [200K](https://docs.anthropic.com/en/docs/about-claude/models) | Proprietary | [Claude.ai](https://claude.ai/) |
| Claude 3.5 Sonnet | 2024/06 | [Claude 3.5 Sonnet](https://docs.anthropic.com/en/docs/about-claude/models) | [Claude 3.5 Sonnet](https://www.anthropic.com/news/claude-3-5-sonnet) | Undisclosed | [200K](https://docs.anthropic.com/en/docs/about-claude/models) | Proprietary | [Claude.ai](https://claude.ai/) |
| Claude 3.7 Sonnet | 2024/10 | [Claude 3.7 Sonnet](https://docs.anthropic.com/en/docs/about-claude/models) | [Claude 3.7 Sonnet](https://www.anthropic.com/news/claude-3-7-sonnet) | Undisclosed | [200K](https://docs.anthropic.com/en/docs/about-claude/models) | Proprietary | [Claude.ai](https://claude.ai/) |
| Gemini Pro | 2023/12 | [Gemini Pro](https://ai.google.dev/models/gemini) | [Introducing Gemini](https://blog.google/technology/ai/google-gemini-ai/) | Undisclosed | [32K](https://ai.google.dev/models/gemini) | Proprietary | [Gemini](https://gemini.google.com/) |
| Gemini 1.5 Pro | 2024/02 | [Gemini 1.5 Pro](https://ai.google.dev/models/gemini) | [Our next-generation model: Gemini 1.5](https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/) | Undisclosed | [1M-2M](https://ai.google.dev/models/gemini) | Proprietary | [Gemini](https://gemini.google.com/) |
| Gemini 2.0 Flash | 2024/12 | [Gemini 2.0 Flash](https://ai.google.dev/gemini-api/docs/models) | [Gemini 2.0 Flash](https://blog.google/technology/google-deepmind/gemini-2-0-flash-multimodal/) | Undisclosed | [1M](https://ai.google.dev/gemini-api/docs/models) | Proprietary | [Gemini](https://gemini.google.com/) |

## Key Developments in 2025

The year 2025 has been marked by several breakthrough releases in the LLM landscape. **Grok 3**, launched by xAI in February 2025, introduced a 1 million token context window and achieved a record-breaking Elo score of 1402 in the Chatbot Arena, making it the first AI model to surpass this milestone. The model was trained on 12.8 trillion tokens and boasts 10x the computational power of its predecessor.

**Meta's Llama 4 family** represents a major leap forward with the introduction of Mixture-of-Experts (MoE) architecture. Llama 4 Scout features an unprecedented 10 million token context window, while Llama 4 Maverick achieves an ELO score of 1417 on LMSYS Chatbot Arena, outperforming GPT-4o and Gemini 2.0 Flash.

**DeepSeek-R1** emerged as the first major open-source reasoning model, trained purely through reinforcement learning without supervised fine-tuning. The model demonstrates performance comparable to OpenAI's o1 across math, code, and reasoning tasks while being completely open-source under the MIT license.

**Qwen 3**, released by Alibaba in April 2025, features a family of "hybrid" reasoning models ranging from 0.6B to 235B parameters, supporting 119 languages and trained on over 36 trillion tokens. The models seamlessly integrate thinking and non-thinking modes, offering users flexibility to control the thinking budget.

**OpenAI** continued its reasoning model series with **o3** and **o4-mini** in April 2025, while **Anthropic** launched **Claude 4** (Opus 4 and Sonnet 4) in May 2025, setting new standards for coding and advanced reasoning with extended thinking capabilities and tool use.

**Google's Gemini 2.5 Pro** debuted as a thinking model with a 1 million token context window, leading on LMArena leaderboards and excelling in coding, math, and multimodal understanding tasks.

## Notable Trends in 2025

1. **Reasoning Models**: The emergence of models that can "think" through problems step-by-step, with extended reasoning capabilities becoming standard.

2. **Massive Context Windows**: Models now support context windows ranging from 1M to 10M tokens, enabling processing of entire codebases and documents.

3. **Mixture-of-Experts (MoE) Architecture**: More efficient model architectures that activate only a subset of parameters during inference.

4. **Open-Source Reasoning**: DeepSeek-R1's success has democratized access to reasoning capabilities previously available only in proprietary models.

5. **Multimodal Integration**: Native multimodality becoming standard, with models trained on text, images, audio, and video from the ground up.

6. **Tool Use and Agentic Capabilities**: Enhanced ability to use tools, execute code, and perform complex multi-step tasks autonomously.

## Performance Benchmarks (2025)

### Reasoning Benchmarks (AIME 2025)
- Grok 3: 93.3%
- DeepSeek-R1-0528: 87.5%
- Gemini 2.5 Pro: 86.7%
- o3-mini: 86.5%

### Coding Benchmarks (SWE-bench Verified)
- Claude Opus 4: 72.5%
- Claude Sonnet 4: 72.7%
- OpenAI Codex 1: 72.1%
- Llama 4 Maverick: ~70%

### Long Context Performance (1M+ tokens)
- Llama 4 Scout: 10M tokens
- Grok 3: 1M tokens
- Gemini 2.5 Pro: 1M tokens
- Llama 4 Maverick: 1M tokens

## Citation

If you find our survey useful for your research, please cite the following paper:

```
@article{hadi2024large,
  title={Large language models: a comprehensive survey of its applications, challenges, limitations, and future prospects},
  author={Hadi, Muhammad Usman and Al Tashi, Qasem and Shah, Abbas and Qureshi, Rizwan and Muneer, Amgad and Irfan, Muhammad and Zafar, Anas and Shaikh, Muhammad Bilal and Akhtar, Naveed and Wu, Jia and others},
  journal={Authorea Preprints},
  year={2024},
  publisher={Authorea}
}
```

---

*Last updated: July 2025*
*Original repository: https://www.techrxiv.org/doi/full/10.36227/techrxiv.23589741.v3*
